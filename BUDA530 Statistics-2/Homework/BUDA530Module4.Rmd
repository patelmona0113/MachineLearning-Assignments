---
title: "BUDA530Module4"
output: word_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1 : 

Using the wage data discussed at length in this section, we want to build a predictive model that will predict two features. 

The first is we want to build a model using non-linear trends and functions to predict wage of an individual. All variables in the ISLR data are available to you and any method you know.

Subset the data such that wages below dollar 250k are the only observations in the data set. Also using these trends look at the prediction accuracy of models developed to predict whether someoneâ€™s salary is above or below (logistic regression), the median ($104k) wage in this data. 


```{r}
library(ISLR)
library(gam)
library(dplyr)
#head(Wage)
#str(Wage)
#summary(Wage)

Wage250less <- filter(Wage,wage < 250)

model1<-lm(wage~age+year+maritl+race+education+jobclass,data=Wage250less)
model2 <- lm(wage~age+year+maritl+education+jobclass,data=Wage250less)
summary(model2)

fit1 <- gam(wage ~ s(age,2) + year + maritl + education + jobclass, data=Wage250less )
fit2 <- gam(wage ~ s(age,2) + s(year,3) + maritl + education + jobclass, data=Wage250less )
fit3 <- gam(wage ~ s(age,3) + s(year,4) + maritl + education + jobclass, data=Wage250less )
fit4 <- gam(wage ~ s(age,4) + s(year,5) + maritl + education + jobclass, data=Wage250less )
fit5 <- gam(wage ~ s(age,5) + s(year,6) + maritl + education + jobclass, data=Wage250less )
fit6 <- gam(wage ~ s(age,6) + s(year,6) + maritl + education + jobclass, data=Wage250less )

anova(fit1,fit2,fit3,fit4,fit5,fit6) 
AIC(fit1,fit2,fit3,fit4,fit5,fit6)

model <- gam(wage ~ s(age,3) + s(year,4) + maritl + education + jobclass, data=Wage250less)
summary(model)
par(mfrow=c(1,3))
plot(model,se = TRUE, col = "green", residuals = TRUE)
plot(wage~age,data=Wage250less)
newdata1 <- data.frame(age=35, year=2006, maritl=c("1. Never Married", "2. Married", "3. Widowed", "4. Divorced", "5. Separated"), education="2. HS Grad", jobclass="1. Industrial", type="Prob")
newdata0<- data.frame(age=18:80, year=2006, maritl="1. Never Married", education="2. HS Grad", jobclass="1. Industrial")
#levels(Wage$jobclass)
p1=predict(model,newdata=newdata0)
lines(18:80,p1, col = "red", lwd=4)
newdata2 <- data.frame(age=60, year=2009, maritl=c("1. Never Married", "2. Married", "3. Widowed", "4. Divorced", "5. Separated"), education="3. Some College", jobclass="2. Information", type="Prob")
predict(model,newdata=newdata2)


#model_logistic <- gam(Wage104Less ~ s(age,3) + s(year,4) + maritl + education + jobclass, data=Wage250less,family=binomial)
#summary(model_logistic)
#par(mfrow=c(1,1))
#plot(model_logistic, se = TRUE, col = "green", residuals = TRUE)
#newdata3 <- data.frame(wage=110, age=35, year=2009, maritl=c("1. Never Married", "2. Married", "3. Widowed", "4. Divorced", "5. Separated"), education="2. HS Grad", jobclass="1. Industrial", type="Prob")
#newdata4 <- data.frame(age=45, year=2009, maritl=("1. Never Married"), education="5. Advanced Degree", jobclass=c("1. Industrial", "2. Information"), type="Prob")
#predict(model_logistic, newdata =newdata3, type="response")

#library(faraway)
#ilogit(predict(model_logistic, newdata =newdata3))
#sum(residuals(model_logistic,type="pearson")^2)/(df.residual(model_logistic)+1)

library(caret)
set.seed(4321)
Wage250less$Wage104Less <- ifelse(Wage250less$wage > 104, 0, 1)
table(Wage250less$Wage104Less)
#1476/2921
trainIndex <- createDataPartition(Wage250less$wage, p=.75, list=FALSE, times=1)
Train <- Wage250less[trainIndex,]
nrow(Train)
Test <- Wage250less[-trainIndex,]
nrow(Test)

modelTrain <- gam(Wage104Less ~ s(age,3) + s(year,4) + maritl + education + jobclass, data=Train,family=binomial)
summary(modelTrain)

predictTest <- predict(modelTrain,newdata=Test, type="response")
summary(predictTest)

table_matrix <- table(Test$Wage104Less,predictTest > 0.5)
sum(diag(table_matrix)) / sum(table_matrix)
sum(residuals(modelTrain,type="pearson")^2)/(df.residual(modelTrain)+1)

```

### Interpretaion :


As per the summary of the different models, variables age, education, maritl status, education and jobclass are signification at p-value level 0.05. Hence, we reject the null hypothesis of these 5 variables. From the ANOVA and AIC results, Model 3 with  age having degree of freedom = 3 and year having df = 4 is prefered. p-value of Model3 < 0.05 hence we reject the null hypothesis. 

The age plot indicates that keeping all the predictors as fixed , wage tends to be highest for intermediate values of age (30-45), and lowest for the very young (<30) and old (>65). 

The year plot indicates that keeping all the predictors Zero, wage increases in the early years and then decreases in 2006 and 2007 and again increases in later years.

The education plot indicates that wage tends to increase with education. In other words, the more educated a person is, the higher their salary.

The marriage status plot indicates that, married workers tend to make the most. The widowed, divorced, and separated groups make a small portion of the workers. The never married workers look like they make less than married workers.

The jobclass plot indicates that information workers tend to have higher wages than industrial workers with other variables set to zero. 

Predicting the probablities of variables at different values,all five variables are making difference on the response and shows similar behaviour as described above.

Logistic Model Prediction accuracy :
As per the Summary of the logistic model, varaibles age,  year, maritl status and education are  significant as p-values are < 0.05. JobClass is significant at level 0.1. All five variables are making difference on the binomial response Wage. 

There are  total 728 observations in test Set, out of which 361 of them are having wages < 104K , and 367 of them are having wages > 104K. In Actual, 50% of the workers are having wages < 104K. While predicting the probablity of test set, 49% of workers are predicted to have wage < 104K. Sigma2 is around 1, model fits the data fairly well.

The model can accurately identify workers with wages above and below 104K wage  with test set accuracy being equal to 69%

